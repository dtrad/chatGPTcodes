{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import sys\n",
    "client = OpenAI(api_key=os.getenv(\"AOAI_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to read text from an ascii file\n",
    "def read_text(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input= read_text(\"inputtext.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "input2=re.sub(r'\\n+', ' ', input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "length=len(input2.split())\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A T RANSFORMER -BASED FRAMEWORK FOR MULTI - VARIATE TIMESERIES REPRESENTATION LEARNING Geo\n",
      "rge Zerveas Brown University Providence, RI, USA george zerveas@brown.eduSrideepika Jayara\n",
      "man, IBM Research Yorktown Heights, NY , USA j.srideepika@ibm.com Dhaval Patel IBM Researc\n",
      "h Yorktown Heights, NY , USA pateldha@us.ibm.comAnuradha Bhamidipaty IBM Research Yorktown\n",
      " Heights, NY , USA anubham@us.ibm.comCarsten Eickhoff Brown University Providence, RI, USA\n",
      " carsten@brown.edu ABSTRACT In this work we propose for the ﬁrst time a transformer-based \n",
      "framework for un- supervised representation learning of multivariate time series. Pre-trai\n",
      "ned models can be potentially used for downstream tasks such as regression and classiﬁcati\n",
      "on, forecasting and missing value imputation. By evaluating our models on several benchmar\n",
      "k datasets for multivariate time series regression and classiﬁcation, we show that our mod\n",
      "eling approach represents the most successful method employ- ing unsupervised learning of \n",
      "multivariate time series presented to date; it is also the ﬁrst unsupervised approach show\n",
      "n to exceed the current state-of-the-art per- formance of supervised methods. It does so b\n",
      "y a signiﬁcant margin, even when the number of training samples is very limited, while off\n",
      "ering computational efﬁ- ciency. Finally, we demonstrate that unsupervised pre-training of\n",
      " our transformer models offers a substantial performance beneﬁt over fully supervised lear\n",
      "ning, even without leveraging additional unlabeled data, i.e., by reusing the same data sa\n",
      "mples through the unsupervised objective. 1 I NTRODUCTION Multivariate time series (MTS) a\n",
      "re an important type of data that is ubiquitous in a wide variety of domains, including sc\n",
      "ience, medicine, ﬁnance, engineering and industrial applications. As the name suggests, th\n",
      "ey typically represent the evolution of a group of synchronous variables (e.g., simultaneo\n",
      "us measurements of different physical quantities) over time, but they can more generally r\n",
      "epresent a group of dependent variables (abscissas) aligned with respect to a common indep\n",
      "endent variable, e.g., absorption spectra under different conditions as a function of ligh\n",
      "t frequency. Despite the recent abundance of MTS data in the much touted era of “Big Data”\n",
      ", the availability of labeled data in particular is far more limited: extensive data label\n",
      "ing is often prohibitively expensive or impractical, as it may require much time and effor\n",
      "t, special infrastructure or domain expertise. For this reason, in all aforementioned doma\n",
      "ins there is great interest in methods which can offer high accuracy by using only a limit\n",
      "ed amount of labeled data or by leveraging the existing plethora of unlabeled data. There \n",
      "is a large variety of modeling approaches for univariate and multivariate time series, wit\n",
      "h deep learning models recently challenging and at times replacing the state of the art in\n",
      " tasks such as forecasting, regression and classiﬁcation (De Brouwer et al., 2019; Tan et \n",
      "al., 2020a; Fawaz et al., 2019b). However, unlike in domains such as Computer Vision or Na\n",
      "tural Language Processing (NLP), the dominance of deep learning for time series is far fro\n",
      "m established: in fact, non-deep learning methods such as TS-CHIEF (Shifaz et al., 2020), \n",
      "HIVE-COTE (Lines et al., 2018), and ROCKET (Dempster et al., 2020) currently hold the reco\n",
      "rd on time series regression and classiﬁca- tion dataset benchmarks (Tan et al., 2020a; Ba\n",
      "gnall et al., 2017), matching or even outperforming 1arXiv:2010.02803v3 [cs.LG] 8 Dec 2020\n",
      "sophisticated deep architectures such as InceptionTime (Fawaz et al., 2019a) and ResNet (F\n",
      "awaz et al., 2019b). In this work, we investigate, for the ﬁrst time, the use of a transfo\n",
      "rmer encoder for unsupervised representation learning of multivariate time series, as well\n",
      " as for the tasks of time series regression and classiﬁcation. Transformers are an importa\n",
      "nt, recently developed class of deep learning models, which were ﬁrst proposed for the tas\n",
      "k of natural language translation (Vaswani et al., 2017) but have since come to monopolize\n",
      " the state-of-the-art performance across virtually all NLP tasks (Raffel et al., 2019). A \n",
      "key factor for the widespread success of transformers in NLP is their aptitude for learnin\n",
      "g how to represent natural language through unsupervised pre-training (Brown et al., 2020;\n",
      " Raffel et al., 2019; Devlin et al., 2018). Besides NLP, transformers have also set the st\n",
      "ate of the art in several domains of sequence generation, such as polyphonic music composi\n",
      "tion (Huang et al., 2018). Transformer models are based on a multi-headed attention mechan\n",
      "ism that offers several key ad- vantages and renders them particularly suitable for time s\n",
      "eries data (see Appendix section A.4 for details). Inspired by the impressive results atta\n",
      "ined through unsupervised pre-training of transformer models in NLP, as our main contribut\n",
      "ion, in the present work we develop a generally applicable method- ology (framework) that \n",
      "can leverage unlabeled data by ﬁrst training a transformer model to extract dense vector r\n",
      "epresentations of multivariate time series through an input denoising (autoregressive) obj\n",
      "ective. The pre-trained model can be subsequently applied to several downstream tasks, suc\n",
      "h as regression, classiﬁcation, imputation, and forecasting. Here, we apply our framework \n",
      "for the tasks of multivariate time series regression and classiﬁcation on several public d\n",
      "atasets and demon- strate that transformer models can convincingly outperform all current \n",
      "state-of-the-art modeling approaches, even when only having access to a very limited amoun\n",
      "t of training data samples (on the order of hundreds of samples), an unprecedented success\n",
      " for deep learning models. To the best of our knowledge, this is also the ﬁrst time that u\n",
      "nsupervised learning has been shown to confer an advantage over supervised learning for cl\n",
      "assiﬁcation and regression of multivariate time series with- out utilizing additional unla\n",
      "beled data samples. Importantly, despite common preconceptions about transformers from the\n",
      " domain of NLP, where top performing models have billions of parameters and require days t\n",
      "o weeks of pre-training on many parallel GPUs or TPUs, we also demonstrate that our models\n",
      ", using at most hundreds of thousands of parameters, can be practically trained even on CP\n",
      "Us; training them on GPUs allows them to be trained as fast as even the fastest and most a\n",
      "ccurate non-deep learning based approaches. 2 R ELATED WORK Regression and classiﬁcation o\n",
      "f time series : Currently, non-deep learning methods such as TS- CHIEF (Shifaz et al., 202\n",
      "0), HIVE-COTE (Lines et al., 2018), and ROCKET (Dempster et al., 2020) constitute the stat\n",
      "e of the art for time series regression and classiﬁcation based on evaluations on public b\n",
      "enchmarks (Tan et al., 2020a;\n"
     ]
    }
   ],
   "source": [
    "# print input2 in lines of 90 characters or less\n",
    "for i in range(0, len(input2), 90):\n",
    "    print(input2[i:i+90])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"you are a assistant that provides the length of a text and a summary of it in the following JSON format:\\n{\\n\\\"Length\\\": <length of the text in words>,\\n\\\"Summary\\\": <short summary of the text, maximum 150 words>,\\n}\\n\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": input2\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "  ],\n",
    "  temperature=1,\n",
    "  max_tokens=256,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.53,\n",
    "  presence_penalty=0,\n",
    "  response_format={\n",
    "    \"type\": \"text\"\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"Length\": 874,\\n  \"Summary\": \"The paper introduces a novel transformer-based framework for unsupervised representation learning of multivariate time series. This approach allows for effective pre-training of models, which can subsequently be used in tasks such as regression, classification, forecasting, and missing value imputation. The authors demonstrate that their method outperforms the current state-of-the-art supervised methods by a significant margin even with limited data. The framework exhibits computational efficiency and can leverage unlabeled data without requiring additional input. The study provides evidence that transformers achieve superior performance in multivariate time series tasks compared to existing methods, setting new benchmarks for deep learning models in this area. The approach overcomes common challenges associated with transformers, like excessive parameter requirements and processing times, making it practical even with CPUs.\"\\n}\\n```'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the response in a readable format with line breaks\n",
    "response.choices[0].message.content\n",
    "#print(response['choices'][0]['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"Length\": 874,\n",
      "  \"Summary\": \"The paper introduces a novel transformer-based framework for unsupervised representation learning of multivariate time series.\n",
      "This approach allows for effective pre-training of models, which can subsequently be used in tasks such as regression, classification, forecasting, and missing value imputation.\n",
      "The authors demonstrate that their method outperforms the current state-of-the-art supervised methods by a significant margin even with limited data.\n",
      "The framework exhibits computational efficiency and can leverage unlabeled data without requiring additional input.\n",
      "The study provides evidence that transformers achieve superior performance in multivariate time series tasks compared to existing methods, setting new benchmarks for deep learning models in this area.\n",
      "The approach overcomes common challenges associated with transformers, like excessive parameter requirements and processing times, making it practical even with CPUs.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# split the previous response into sentences\n",
    "import re\n",
    "sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', response.choices[0].message.content)\n",
    "for s in sentences:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
