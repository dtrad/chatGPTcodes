{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()  # Get process rank (ID)\n",
    "size = comm.Get_size()  # Get the total number of processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from rank 0 of 1\n",
      "PyTorch version: 2.3.1\n",
      "size of the mpi comm = 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hello from rank {rank} of {size}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f'size of the mpi comm = {size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 received chunk: tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.,\n",
      "        15., 16.])\n",
      "Gathered tensor after multiplication: tensor([ 2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20., 22., 24., 26., 28.,\n",
      "        30., 32.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define the size of the tensor and scalar for computation\n",
    "tensor_size = 16\n",
    "scalar = 2.0\n",
    "\n",
    "if rank == 0:\n",
    "    # Rank 0 will initialize the full tensor and scatter it\n",
    "    full_tensor = torch.arange(1, tensor_size + 1, dtype=torch.float32)  # Create a tensor from 1 to tensor_size\n",
    "else:\n",
    "    full_tensor = None\n",
    "\n",
    "# Each process gets a chunk of the tensor\n",
    "chunk_size = tensor_size // size\n",
    "local_chunk = torch.zeros(chunk_size, dtype=torch.float32)\n",
    "\n",
    "# Scatter the tensor from rank 0 to all processes\n",
    "comm.Scatter(full_tensor, local_chunk, root=0)\n",
    "\n",
    "print(f\"Rank {rank} received chunk: {local_chunk}\")\n",
    "\n",
    "# Each process performs a computation on its local chunk (e.g., multiply by a scalar)\n",
    "local_chunk *= scalar\n",
    "\n",
    "# Gather the results back at rank 0\n",
    "gathered_tensor = None\n",
    "if rank == 0:\n",
    "    gathered_tensor = torch.zeros(tensor_size, dtype=torch.float32)\n",
    "\n",
    "comm.Gather(local_chunk, gathered_tensor, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"Gathered tensor after multiplication: {gathered_tensor}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
