{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9aa2b3",
   "metadata": {},
   "source": [
    "# Understanding Agents vs Prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a558f737",
   "metadata": {},
   "source": [
    "Link to chatGPT discussion: https://chatgpt.com/share/683650cc-0450-8001-a54e-0755978d8b1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c845a055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agents\n"
     ]
    }
   ],
   "source": [
    "# displays the conda environment used by the current process\n",
    "!conda info --envs | grep '*' | awk '{print $1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f3298",
   "metadata": {},
   "source": [
    "### Is the name Agents in GAI the same as writing a request to LLMs using prompt engineering  \n",
    "### and parameters as temperature, top k, role, user and system?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12858d",
   "metadata": {},
   "source": [
    "Great question. The term **\"Agents\"** in the context of **Generative AI (GAI)**‚Äîespecially as used by OpenAI and similar frameworks‚Äîis related to, but not the same as, simply sending a request to an LLM with a well-crafted prompt and parameters like temperature, top-k, role, etc.\n",
    "\n",
    "### Here's the key difference:\n",
    "\n",
    "#### 1. **Prompting an LLM**\n",
    "\n",
    "This involves:\n",
    "\n",
    "* A single input (or a chat history) with roles (`system`, `user`, `assistant`)\n",
    "* Prompt engineering (writing effective instructions)\n",
    "* Parameters like `temperature`, `top_p`, `max_tokens`, etc.\n",
    "* No persistent memory or external tool usage (unless manually coded)\n",
    "\n",
    "This is **stateless**: each request is independent unless you manually pass conversation history.\n",
    "\n",
    "#### 2. **Agents in GAI (like OpenAI‚Äôs GPTs or LangChain Agents)**\n",
    "\n",
    "This involves:\n",
    "\n",
    "* An **LLM + memory + tools + planning + acting**\n",
    "* Capable of **autonomous decision-making**, like:\n",
    "\n",
    "  * Figuring out sub-tasks\n",
    "  * Calling tools/APIs/web searches\n",
    "  * Keeping track of state or objectives\n",
    "* Can use **multi-turn reasoning** and **external actions** (like searching the web or running Python code)\n",
    "* Often built on **frameworks** like LangChain, OpenAI's `function calling`, ReAct, or OpenAI's GPT-based \"Agents\" system\n",
    "\n",
    "This is **stateful** and **autonomous**: it can manage tasks that involve planning, tool use, and remembering context over time.\n",
    "\n",
    "---\n",
    "\n",
    "### Analogy\n",
    "\n",
    "* **Prompting an LLM** is like giving a very smart assistant a one-time instruction.\n",
    "* **Using an Agent** is like hiring a capable intern who can:\n",
    "\n",
    "  * Understand goals\n",
    "  * Ask clarifying questions\n",
    "  * Use tools like calculators, databases, or browsers\n",
    "  * Keep track of progress over time\n",
    "  * Make decisions along the way\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "**No, \"Agents\" is not the same as just prompt engineering with parameters**‚Äîit's a more advanced construct that combines LLMs with structured decision-making, memory, and sometimes tool use.\n",
    "\n",
    "If you're doing simple prompting with temperature/top-k/roles, you're not yet using Agents‚Äîbut you can *build* an Agent by combining those capabilities in a structured way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f74a41",
   "metadata": {},
   "source": [
    "## Prompting Example (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc9baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "A Python list comprehension is a compact way of creating a Python list. It's a syntactic construct that enables lists to be created from other lists (or any iterable) by applying an expression to each element in the original list.\n",
      "\n",
      "The basic syntax of a list comprehension is:\n",
      "```python\n",
      "[expression for element in iterable]\n",
      "```\n",
      "Here, `expression` is a Python expression that is applied to each `element` in the `iterable`.\n",
      "\n",
      "Here's an example of a list comprehension that creates a new list containing the squares of all numbers in an existing list:\n",
      "\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "squares = [n**2 for n in numbers]\n",
      "```\n",
      "After running this code, `squares` would be `[1, 4, 9, 16, 25]`.\n",
      "\n",
      "You can also add an optional `if` condition to a list comprehension to filter the elements from the original list that you want to include in the new list. Here's an example:\n",
      "\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "even_squares = [n**2 for n in numbers if n % 2 == 0]\n",
      "```\n",
      "After running this code, `even_squares` would be `[4, 16]`, because it only includes the squares of the even numbers from the original list.\n"
     ]
    }
   ],
   "source": [
    "#%pip install --quiet openai\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that explains Python code.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain what a Python list comprehension is.\"}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11509f45",
   "metadata": {},
   "source": [
    "## Agent Example (LangChain with Tool Use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333d364b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63798/3105818160.py:17: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n",
      "/tmp/ipykernel_63798/3105818160.py:25: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  agent.run(\"What is the square root of 8461 divided by 3?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use a calculator to solve this problem.\n",
      "Action: Calculator\n",
      "Action Input: square root of 8461 divided by 3\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 30.661231402393337\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: 30.661231402393337\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'30.661231402393337'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%pip install numexpr\n",
    "\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Load tools, like a calculator\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Create a reasoning agent that can use tools\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Ask a question that requires tool use\n",
    "agent.run(\"What is the square root of 8461 divided by 3?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a35ec",
   "metadata": {},
   "source": [
    "## Agent with Memory and Tools (LangChain)\n",
    "Let's build on the agent example by adding memory and tool use, using LangChain. This agent will:\n",
    "\n",
    "Remember previous interactions\n",
    "\n",
    "Use tools (like a calculator or a text search tool)\n",
    "\n",
    "Work over multiple turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92cc894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Calculator\n",
      "Action Input: What is the 10th Fibonacci number?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 55.00363612324741\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: That's interesting! Fibonacci numbers are a fascinating mathematical concept. Did you know that they were first introduced by Leonardo Fibonacci in the 12th century? They have many interesting properties and applications in fields such as computer science and finance. Is there anything specific you would like to know about Fibonacci numbers?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Calculator\n",
      "Action Input: 20th Fibonacci number\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 6765.000029563931\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: The 20th Fibonacci number is 6765.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Sure, you mentioned that you were working on a project involving Fibonacci numbers. Is there anything else you would like to know about them?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure, you mentioned that you were working on a project involving Fibonacci numbers. Is there anything else you would like to know about them?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# üîê Set your OpenAI API key\n",
    "api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(f\"Using OpenAI API key: {api_key}\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Load tools (e.g., calculator)\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Create an agent with memory and tool use\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# üó®Ô∏è Multi-turn interaction\n",
    "agent.run(\"Hi, I‚Äôm working on a project involving Fibonacci numbers.\")\n",
    "\n",
    "agent.run(\"What is the 20th Fibonacci number?\")\n",
    "\n",
    "agent.run(\"Can you remind me what I said earlier?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f6b41c",
   "metadata": {},
   "source": [
    "## Example 1: File Upload + Summarization Agent\n",
    "This agent will:\n",
    "\n",
    "Read a local file (e.g. a .txt or .pdf)\n",
    "\n",
    "Summarize it using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04503df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69343/2018193396.py:26: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(), retriever=retriever)\n",
      "/tmp/ipykernel_69343/2018193396.py:29: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(\"Summarize this document for me.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The document discusses the development of a methodology that can use unlabeled data to extract dense vector representations of time series through input denoising. It also explores the impact of additional labels on performance and the potential use of different masking patterns for different objectives. Future work includes a quantitative comparison with existing methods.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Set your OpenAI key\n",
    "api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Step 1: Load a PDF file\n",
    "loader = PyPDFLoader(\"./paper.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Create vector index\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Step 4: Build a QA system with retrieval\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(), retriever=retriever)\n",
    "\n",
    "# Step 5: Ask questions or summarize\n",
    "response = qa_chain.run(\"Summarize this document for me.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac2424",
   "metadata": {},
   "source": [
    "## Example 2: Web Search Agent + Memory\n",
    "This agent:\n",
    "\n",
    "Has memory\n",
    "\n",
    "Can access the internet (via Tavily tool)\n",
    "\n",
    "Remembers what you‚Äôve asked across turns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fce428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "tavily_key=os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"tavily-search\"], llm=llm)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Ask questions across turns\n",
    "agent.run(\"What is the latest news about quantum computing?\")\n",
    "agent.run(\"Summarize that for a high school student.\")\n",
    "agent.run(\"Remind me what I just asked you.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
