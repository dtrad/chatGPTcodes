{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def pdf_to_text(pdf_path, txt_path):\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        with open(txt_path, 'w') as txt_file:\n",
    "            for page in reader.pages:\n",
    "                txt_file.write(page.extract_text())\n",
    "\n",
    "pdf_to_text('paper.pdf', 'output.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A T RANSFORMER -BASED FRAMEWORK FOR MULTI - VARIATE TIMESERIES REPRESENTATION LEARNING George Zerveas Brown University Providence, RI, USA george zerveas@brown.eduSrideepika Jayaraman, IBM Research Yorktown Heights, NY , USA j.srideepika@ibm.com Dhaval Patel IBM Research Yorktown Heights, NY , USA pateldha@us.ibm.comAnuradha Bhamidipaty IBM Research Yorktown Heights, NY , USA anubham@us.ibm.comCarsten Eickhoff Brown University Providence, RI, USA carsten@brown.edu ABSTRACT In this work we propose for the ﬁrst time a transformer-based framework for un- supervised representation learning of multivariate time series. Pre-trained models can be potentially used for downstream tasks such as regression and classiﬁcation, forecasting and missing value imputation. By evaluating our models on several benchmark datasets for multivariate time series regression and classiﬁcation, we show that our modeling approach represents the most successful method employ- ing unsupervised learning of multivariate time series presented to date; it is also the ﬁrst unsupervised approach shown to exceed the current state-of-the-art per- formance of supervised methods. It does so by a signiﬁcant margin, even when the number of training samples is very limited, while offering computational efﬁ- ciency. Finally, we demonstrate that unsupervised pre-training of our transformer models offers a substantial performance beneﬁt over fully supervised learning, even without leveraging additional unlabeled data, i.e., by reusing the same data samples through the unsupervised objective. 1 I NTRODUCTION Multivariate time series (MTS) are an important type of data that is ubiquitous in a wide variety of domains, including science, medicine, ﬁnance, engineering and industrial applications. As the name suggests, they typically represent the evolution of a group of synchronous variables (e.g., simultaneous measurements of different physical quantities) over time, but they can more generally represent a group of dependent variables (abscissas) aligned with respect to a common independent variable, e.g., absorption spectra under different conditions as a function of light frequency. Despite the recent abundance of MTS data in the much touted era of “Big Data”, the availability of labeled data in particular is far more limited: extensive data labeling is often prohibitively expensive or impractical, as it may require much time and effort, special infrastructure or domain expertise. For this reason, in all aforementioned domains there is great interest in methods which can offer high accuracy by using only a limited amount of labeled data or by leveraging the existing plethora of unlabeled data. There is a large variety of modeling approaches for univariate and multivariate time series, with deep learning models recently challenging and at times replacing the state of the art in tasks such as forecasting, regression and classiﬁcation (De Brouwer et al., 2019; Tan et al., 2020a; Fawaz et al., 2019b). However, unlike in domains such as Computer Vision or Natural Language Processing (NLP), the dominance of deep learning for time series is far from established: in fact, non-deep learning methods such as TS-CHIEF (Shifaz et al., 2020), HIVE-COTE (Lines et al., 2018), and ROCKET (Dempster et al., 2020) currently hold the record on time series regression and classiﬁca- tion dataset benchmarks (Tan et al., 2020a; Bagnall et al., 2017), matching or even outperforming 1arXiv:2010.02803v3 [cs.LG] 8 Dec 2020sophisticated deep architectures such as InceptionTime (Fawaz et al., 2019a) and ResNet (Fawaz et al., 2019b). In this work, we investigate, for the ﬁrst time, the use of a transformer encoder for unsupervised representation learning of multivariate time series, as well as for the tasks of time series regression and classiﬁcation. Transformers are an important, recently developed class of deep learning models, which were ﬁrst proposed for the task of natural language translation (Vaswani et al., 2017) but have since come to monopolize the state-of-the-art performance across virtually all NLP tasks (Raffel et al., 2019). A key factor for the widespread success of transformers in NLP is their aptitude for learning how to represent natural language through unsupervised pre-training (Brown et al., 2020; Raffel et al., 2019; Devlin et al., 2018). Besides NLP, transformers have also set the state of the art in several domains of sequence generation, such as polyphonic music composition (Huang et al., 2018). Transformer models are based on a multi-headed attention mechanism that offers several key ad- vantages and renders them particularly suitable for time series data (see Appendix section A.4 for details). Inspired by the impressive results attained through unsupervised pre-training of transformer models in NLP, as our main contribution, in the present work we develop a generally applicable method- ology (framework) that can leverage unlabeled data by ﬁrst training a transformer model to extract dense vector representations of multivariate time series through an input denoising (autoregressive) objective. The pre-trained model can be subsequently applied to several downstream tasks, such as regression, classiﬁcation, imputation, and forecasting. Here, we apply our framework for the tasks of multivariate time series regression and classiﬁcation on several public datasets and demon- strate that transformer models can convincingly outperform all current state-of-the-art modeling approaches, even when only having access to a very limited amount of training data samples (on the order of hundreds of samples), an unprecedented success for deep learning models. To the best of our knowledge, this is also the ﬁrst time that unsupervised learning has been shown to confer an advantage over supervised learning for classiﬁcation and regression of multivariate time series with- out utilizing additional unlabeled data samples. Importantly, despite common preconceptions about transformers from the domain of NLP, where top performing models have billions of parameters and require days to weeks of pre-training on many parallel GPUs or TPUs, we also demonstrate that our models, using at most hundreds of thousands of parameters, can be practically trained even on CPUs; training them on GPUs allows them to be trained as fast as even the fastest and most accurate non-deep learning based approaches. 2 R ELATED WORK Regression and classiﬁcation of time series : Currently, non-deep learning methods such as TS- CHIEF (Shifaz et al., 2020), HIVE-COTE (Lines et al., 2018), and ROCKET (Dempster et al., 2020) constitute the state of the art for time series regression and classiﬁcation based on evaluations on public benchmarks (Tan et al., 2020a;\n"
     ]
    }
   ],
   "source": [
    "# select the first 1000 words of the output.txt file\n",
    "with open('output.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    words = text.split()[:1000]\n",
    "    print(' '.join(words))\n",
    "\n",
    "# save in the file inputtext.txt\n",
    "with open('inputtext.txt', 'w') as file:\n",
    "    file.write(' '.join(words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
