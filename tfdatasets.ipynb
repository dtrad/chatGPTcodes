{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with datasets in Tensorflow\n",
    "## The tf.keras.datasets vs the Tensorflow Datasets\n",
    "This is intended as an introductory ML class on datasets (to be merged with datagenerators)\n",
    "Daniel Trad, using chatGPT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.keras.datasets module provides access to a number of public datasets as tf.data.Dataset objects, which are easy to use with tf.keras models. These datasets are small and well-understood, and are useful for testing and debugging.\n",
    "\n",
    "tfds (TensorFlow Datasets) is a collection of datasets ready to use with TensorFlow. It includes a wide range of datasets for various tasks such as object detection, language translation, and recommendation systems. The datasets provided by tfds are typically larger and more complex than those in tf.keras.datasets. They are also well-documented and include detailed information about the data, such as the number of classes, the format of the data, and how the data was collected and preprocessed. Additionally, tfds includes tools for loading, preprocessing, and manipulating the data, making it easier to work with large and complex datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of how you can create a deep neural network (DNN) in tf.keras for the MNIST dataset using tf.keras.datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 08:57:50.570562: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-06 08:57:51.742583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10226 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:09:00.0, compute capability: 8.6\n",
      "2023-01-06 08:57:51.743118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 5859 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:05:00.0, compute capability: 7.5\n",
      "2023-01-06 08:57:52.181692: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  29/1875 [..............................] - ETA: 3s - loss: 1.6431 - accuracy: 0.5603   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 08:57:53.570769: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 1ms/step - loss: 0.2265 - accuracy: 0.9334\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0968 - accuracy: 0.9707\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0674 - accuracy: 0.9786\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0508 - accuracy: 0.9838\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0392 - accuracy: 0.9870\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07693281024694443, 0.9782999753952026]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Build the model\n",
    "inputs = tf.keras.layers.Input(shape=(28, 28))\n",
    "x = Flatten()(inputs)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will create a simple DNN with two hidden layers (128 units each) and an output layer with 10 units, corresponding to the 10 classes in the MNIST dataset. The model is then compiled using the adam optimizer and the sparse_categorical_crossentropy loss function, and is trained using the training data for 5 epochs. Finally, the model is evaluated on the test data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of how you can create a deep neural network (DNN) in tf.keras for the MNIST dataset using tfds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2371 - accuracy: 0.9301\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1002 - accuracy: 0.9693\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0682 - accuracy: 0.9792\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0483 - accuracy: 0.9851\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0368 - accuracy: 0.9880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f40ba074700>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Load the MNIST dataset\n",
    "ds, info = tfds.load('mnist', split=['train', 'test'], as_supervised=True)\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess(image, label):\n",
    "  image = tf.cast(image, tf.float32) / 255.0\n",
    "  return image, label\n",
    "\n",
    "ds = ds.map(preprocess)\n",
    "ds = ds.batch(32)\n",
    "ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Build the model\n",
    "inputs = tf.keras.layers.Input(shape=(28, 28))\n",
    "x = Flatten()(inputs)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(ds, epochs=5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will create a simple DNN with two hidden layers (128 units each) and an output layer with 10 units, corresponding to the 10 classes in the MNIST dataset. The data is loaded using tfds.load and preprocessed by normalizing the pixel values and batching the data. The model is then compiled using the adam optimizer and the sparse_categorical_crossentropy loss function, and is trained using the training data for 5 epochs. Note that the test data is not used in this example.\n",
    "\n",
    "To use the test data to evaluate the model, you can use the ds_test dataset created in the same way as ds, and call model.evaluate on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50e0cab8bd62756c7fc66a63f77f1d7a5658785ded0a9e3406fe8e8350515108"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
